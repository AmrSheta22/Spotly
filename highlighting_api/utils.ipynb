{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xxksOT9CbxE",
        "outputId": "ba5290fd-1e36-41ac-f176-783658c156de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 15.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 54.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 75.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.2 transformers-4.24.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iUFaBazTBeoE"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel, utils\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HRVWnV9EBzEc"
      },
      "outputs": [],
      "source": [
        "def load_model_tokenizer(model_path):\n",
        "  model = AutoModel.from_pretrained(model_path, output_attentions=True)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "  return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1OkOoYNkj1Gz"
      },
      "outputs": [],
      "source": [
        "def text_tokenization(input_text, model, tokenizer):\n",
        "  batch_encoding = tokenizer.encode_plus(input_text, return_tensors='pt')\n",
        "  tokenized_inputs = batch_encoding[\"input_ids\"]\n",
        "  outputs = model(tokenized_inputs)  # Run model\n",
        "  attention = outputs[-1]  # Retrieve attention from model outputs\n",
        "  return attention, tokenized_inputs, batch_encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Jg2ryq7ehctx"
      },
      "outputs": [],
      "source": [
        "def calculate_total_attention(attention):\n",
        "  layer_sums = np.zeros((1, attention[0][0][0].shape[0]))\n",
        "  for layer in attention:\n",
        "    head_sums = np.zeros((1, layer[0][0].shape[1]))\n",
        "    for head in layer[0]:\n",
        "      head = head.detach().numpy()\n",
        "      head_sums +=np.sum(head, axis = 0)\n",
        "    layer_sums += head_sums\n",
        "  return layer_sums[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "o3CKZr8IeAvC"
      },
      "outputs": [],
      "source": [
        "def filter_tokens(inputs, layer_sums):\n",
        "  ids = inputs[0].detach().numpy()\n",
        "  out = [101, 102, 1010, 1011, 1012, 100, 1005, 1025, 1000]\n",
        "  mask1 = np.ones(ids.shape, dtype = bool)\n",
        "  for i in range(len(mask1)):\n",
        "    if ids[i] in out:\n",
        "      mask1[i] = 0\n",
        "  ids = ids[mask1]\n",
        "  layer_sums = layer_sums[mask1]\n",
        "  return ids, layer_sums, mask1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YYM85ewJBBlB"
      },
      "outputs": [],
      "source": [
        "def arbitrary_threshold(layer_sums, ids, threshold = 1.2):\n",
        "  mean = np.mean(layer_sums)\n",
        "  mask2 = np.zeros(layer_sums.shape, dtype = bool)\n",
        "  for i, k in enumerate(layer_sums):\n",
        "    if k > threshold*mean:\n",
        "      mask2[i] = 1\n",
        "  ids = ids[mask2]\n",
        "  layer_sums = layer_sums[mask2]\n",
        "  return ids, layer_sums, mask2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-vmj12cdMODg"
      },
      "outputs": [],
      "source": [
        "def get_word_indices(mask1, mask2):\n",
        "  indices = np.arange(0,len(mask1))\n",
        "  indices= indices[mask1]\n",
        "  indices= indices[mask2]\n",
        "  return indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "10rvhYkZM8Q4"
      },
      "outputs": [],
      "source": [
        "def get_corresponding_spans(batch_encoding, indices):\n",
        "  all_spans = []\n",
        "  for i in indices:\n",
        "    all_spans.append([batch_encoding.token_to_chars(i)[0], batch_encoding.token_to_chars(i)[1]])\n",
        "  return all_spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "l35fm3PJVms_"
      },
      "outputs": [],
      "source": [
        "def spans_to_words(all_spans, input_text):\n",
        "  words = []\n",
        "  for i in all_spans:\n",
        "    words.append(input_text[i[0]:i[1]])\n",
        "  return words"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "17cacb68b189fc8317fc342c9b4062609edce64895a53fd14dc04e71472cd54d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
